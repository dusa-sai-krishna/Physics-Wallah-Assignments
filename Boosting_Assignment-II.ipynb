{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1\n",
    "\n",
    "Gradient Boosting Regression is an ensemble machine learning technique that builds a predictive model by combining the predictions of multiple weak models (typically decision trees) in a sequential manner. It minimizes the error of the model by continuously adjusting the model's parameters based on the gradient of the loss function, hence the name \"gradient boosting.\" This process results in a strong predictive model that is particularly effective for regression tasks, where the goal is to predict a continuous numeric target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2\n",
    "\n",
    "\n",
    "\n",
    "Creating a complete gradient boosting algorithm from scratch is a complex task, but I can provide a simplified example using Python and NumPy to give you a basic idea. In practice, libraries like scikit-learn or XGBoost are recommended for actual use. Here's a simplified code example for gradient boosting regression:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate a simple dataset\n",
    "X = np.arange(0, 10, 0.1).reshape(-1, 1)\n",
    "y = 2 * X + np.random.normal(0, 0.5, X.shape[0])\n",
    "\n",
    "# Number of estimators (trees)\n",
    "n_estimators = 100\n",
    "\n",
    "# Learning rate (shrinkage)\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Initialize predictions\n",
    "predictions = np.zeros_like(y)\n",
    "\n",
    "# Build the ensemble of trees\n",
    "for _ in range(n_estimators):\n",
    "    # Calculate residuals\n",
    "    residuals = y - predictions\n",
    "\n",
    "    # Fit a decision tree to the residuals\n",
    "    tree = DecisionTreeRegressor(max_depth=2)\n",
    "    tree.fit(X, residuals)\n",
    "\n",
    "    # Make predictions with the current tree\n",
    "    tree_predictions = tree.predict(X)\n",
    "\n",
    "    # Update predictions with the tree's predictions, weighted by the learning rate\n",
    "    predictions += learning_rate * tree_predictions\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y, predictions)\n",
    "r2 = r2_score(y, predictions)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "In this simplified example:\n",
    "\n",
    "We generate a synthetic dataset with a linear relationship plus noise.\n",
    "We create an ensemble of decision trees (100 trees in this case) and update predictions in each iteration.\n",
    "We calculate metrics like Mean Squared Error and R-squared to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.003960766652110565\n",
      "R-squared: 0.9998811651169486\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate a simple dataset\n",
    "X = np.arange(0, 10, 0.1).reshape(-1, 1)\n",
    "y = 2 * X + np.random.normal(0, 0.5, X.shape[0])\n",
    "\n",
    "# Number of estimators (trees)\n",
    "n_estimators = 100\n",
    "\n",
    "# Learning rate (shrinkage)\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Initialize predictions\n",
    "predictions = np.zeros_like(y)\n",
    "\n",
    "# Build the ensemble of trees\n",
    "for _ in range(n_estimators):\n",
    "    # Calculate residuals\n",
    "    residuals = y - predictions\n",
    "\n",
    "    # Fit a decision tree to the residuals\n",
    "    tree = DecisionTreeRegressor(max_depth=2)\n",
    "    tree.fit(X, residuals)\n",
    "\n",
    "    # Make predictions with the current tree\n",
    "    tree_predictions = tree.predict(X)\n",
    "\n",
    "    # Update predictions with the tree's predictions, weighted by the learning rate\n",
    "    predictions += learning_rate * tree_predictions\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y, predictions)\n",
    "r2 = r2_score(y, predictions)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3\n",
    "\n",
    "#tips dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "      <th>tip</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoker</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.99</td>\n",
       "      <td>1.01</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.34</td>\n",
       "      <td>1.66</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_bill   tip     sex smoker  day    time  size\n",
       "0       16.99  1.01  Female     No  Sun  Dinner     2\n",
       "1       10.34  1.66    Male     No  Sun  Dinner     3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset\n",
    "tips=sns.load_dataset('tips')\n",
    "df=tips.copy()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder=LabelEncoder()\n",
    "df['encoded_sex']=encoder.fit_transform(df['sex'])\n",
    "df['encoded_smoker']=encoder.fit_transform(df['smoker'])\n",
    "df['encoded_day']=encoder.fit_transform(df['day'])\n",
    "df['encoded_time']=encoder.fit_transform(df['time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df[['total_bill', 'tip', 'size','encoded_sex', 'encoded_smoker', 'encoded_day']]\n",
    "y=df['encoded_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "      <th>tip</th>\n",
       "      <th>size</th>\n",
       "      <th>encoded_sex</th>\n",
       "      <th>encoded_smoker</th>\n",
       "      <th>encoded_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.99</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.34</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.01</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.68</td>\n",
       "      <td>3.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.59</td>\n",
       "      <td>3.61</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_bill   tip  size  encoded_sex  encoded_smoker  encoded_day\n",
       "0       16.99  1.01     2            0               0            2\n",
       "1       10.34  1.66     3            1               0            2\n",
       "2       21.01  3.50     3            1               0            2\n",
       "3       23.68  3.31     2            1               0            2\n",
       "4       24.59  3.61     4            0               0            2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "parameters={\n",
    "    'learning_rate':np.linspace(0,1,3),\n",
    "    'n_estimators':[100,500],\n",
    "    'max_depth':np.arange(1,10)\n",
    "}\n",
    "classifier=GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridcv=GridSearchCV(classifier,param_grid=parameters,scoring='accuracy',verbose=3,n_jobs=-1,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=-1,\n",
       "             param_grid={&#x27;learning_rate&#x27;: array([0. , 0.5, 1. ]),\n",
       "                         &#x27;max_depth&#x27;: array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       "                         &#x27;n_estimators&#x27;: [100, 500]},\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=-1,\n",
       "             param_grid={&#x27;learning_rate&#x27;: array([0. , 0.5, 1. ]),\n",
       "                         &#x27;max_depth&#x27;: array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       "                         &#x27;n_estimators&#x27;: [100, 500]},\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_jobs=-1,\n",
       "             param_grid={'learning_rate': array([0. , 0.5, 1. ]),\n",
       "                         'max_depth': array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       "                         'n_estimators': [100, 500]},\n",
       "             scoring='accuracy', verbose=3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridcv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.5, 'max_depth': 2, 'n_estimators': 500}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.964102564102564"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridcv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4\n",
    "\n",
    "A weak learner in Gradient Boosting is a simple and relatively low-performing model that is used as a base model in the ensemble. Weak learners are typically decision trees with limited depth or other simple algorithms. In the context of boosting, these models don't need to be highly accurate on their own. Instead, they serve as building blocks that, when combined, contribute to the creation of a strong predictive model. The boosting algorithm focuses on improving the areas where the weak learners perform poorly, gradually converting them into a strong ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5\n",
    "\n",
    "The intuition behind the Gradient Boosting algorithm is to iteratively improve model predictions by focusing on the errors made by the current model. Here's a simplified intuition:\n",
    "\n",
    "1. Start with a simple model (typically a weak learner) to make predictions.\n",
    "2. Calculate the errors or residuals by comparing the model's predictions to the actual target values.\n",
    "3. Build a new model that tries to correct these errors by learning from them.\n",
    "4. Add the new model's predictions to the previous model's predictions.\n",
    "5. Repeat this process for a specified number of iterations or until a performance criterion is met.\n",
    "6. The ensemble of models gradually reduces the errors, creating a strong learner from the combination of weak learners.\n",
    "\n",
    "In essence, Gradient Boosting focuses on areas where the current model is making mistakes and incrementally improves its predictions. It does this by optimizing a loss function, which quantifies the difference between the model's predictions and the actual values. The algorithm uses the gradient of this loss function to guide the updates, hence the name \"Gradient Boosting.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6\n",
    "\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential and additive manner. Here's how it works:\n",
    "\n",
    "1. **Initialization**: Start with a simple model (typically a decision tree with limited depth) as the first weak learner. This initial model makes predictions.\n",
    "\n",
    "2. **Compute Residuals**: Calculate the residuals (the differences between the model's predictions and the actual target values) for each data point in the training set.\n",
    "\n",
    "3. **Build a New Weak Learner**: Train a new weak learner on the residuals from the previous step. This new learner is designed to correct the errors or residuals made by the previous model.\n",
    "\n",
    "4. **Update Predictions**: Add the predictions of the new weak learner to the predictions made by the previous models. The ensemble's predictions are updated to account for the new learner's contribution.\n",
    "\n",
    "5. **Repeat**: Steps 2 to 4 are repeated for a specified number of iterations (controlled by the number of estimators) or until a stopping criterion is met.\n",
    "\n",
    "6. **Final Prediction**: The final prediction of the Gradient Boosting ensemble is the cumulative sum of the predictions made by each weak learner.\n",
    "\n",
    "The algorithm assigns a weight to each weak learner, based on their performance. Learners that correct errors more effectively receive higher weights. This way, the ensemble focuses on the areas where the previous models underperformed, gradually improving overall predictions.\n",
    "\n",
    "The power of Gradient Boosting lies in its ability to convert a collection of weak models into a strong learner that minimizes the loss function and makes accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7\n",
    "\n",
    "The mathematical intuition of the Gradient Boosting algorithm involves the following key steps:\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Start with an initial prediction, often a simple constant, like the mean of the target values.\n",
    "   - Calculate the residuals by subtracting this initial prediction from the actual target values.\n",
    "\n",
    "2. **Fitting Weak Learners**:\n",
    "   - Fit a weak learner (e.g., a decision tree) to the residuals.\n",
    "   - The weak learner aims to capture the patterns in the residuals, i.e., the errors made by the current prediction.\n",
    "\n",
    "3. **Weighted Contributions**:\n",
    "   - Assign a weight (learning rate) to the predictions of the weak learner. The weight determines how much of the weak learner's predictions are added to the overall prediction.\n",
    "   - Update the overall prediction by adding the weighted predictions of the weak learner.\n",
    "\n",
    "4. **Iterative Process**:\n",
    "   - Repeat steps 2 and 3 for a predefined number of iterations (number of weak learners) or until a performance criterion is met.\n",
    "   - In each iteration, the algorithm focuses on the residuals left by the previous models, gradually reducing the prediction errors.\n",
    "\n",
    "5. **Final Prediction**:\n",
    "   - The final prediction is the cumulative sum of the weighted predictions from all the weak learners.\n",
    "   - This ensemble prediction minimizes a loss function (e.g., mean squared error) by adjusting the weights and parameters of the weak learners.\n",
    "\n",
    "The mathematical intuition behind Gradient Boosting relies on optimizing a loss function by iteratively adding weak learners and adjusting their contributions to create a strong ensemble model. The algorithm updates the weights, or \"boosts\" the performance, of the individual learners to improve the overall prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
