{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1\n",
    "\n",
    "Min-Max scaling is a data preprocessing technique used to rescale numerical features in a dataset to a specific range, typically between 0 and 1. It is achieved by subtracting the minimum value in the feature from each data point and then dividing the result by the range (the difference between the maximum and minimum values).\n",
    "\n",
    "Mathematically, for a feature \"x,\" Min-Max scaling can be expressed as:\n",
    "\n",
    "normalized(x) = {x - min(x)}/{max(x) - min(x)}\n",
    "\n",
    "Here's a brief example to illustrate Min-Max scaling:\n",
    "\n",
    "Suppose you have a dataset of ages with the following values:\n",
    "\\[ [25, 30, 35, 40, 45, 50] \\]\n",
    "\n",
    "To scale these ages using Min-Max scaling, you would:\n",
    "\n",
    "1. Find the minimum value (min(x)) in the dataset, which is 25.\n",
    "2. Find the maximum value (max(x)) in the dataset, which is 50.\n",
    "3. Apply the formula for Min-Max scaling to each age:\n",
    "   - For 25: \\[ \\frac{25 - 25}{50 - 25} = 0 \\]\n",
    "   - For 30: \\[ \\frac{30 - 25}{50 - 25} = 0.25 \\]\n",
    "   - For 35: \\[ \\frac{35 - 25}{50 - 25} = 0.5 \\]\n",
    "   - For 40: \\[ \\frac{40 - 25}{50 - 25} = 0.75 \\]\n",
    "   - For 45: \\[ \\frac{45 - 25}{50 - 25} = 1.0 \\]\n",
    "   - For 50: \\[ \\frac{50 - 25}{50 - 25} = 1.0 \\]\n",
    "\n",
    "After Min-Max scaling, the scaled ages will be in the range [0, 1], making them suitable for machine learning algorithms that are sensitive to the scale of the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2\n",
    "\n",
    "The Unit Vector technique in feature scaling, often referred to as \"Normalization,\" is a data preprocessing method used to scale numerical features in a dataset to have a unit norm or length. In other words, it transforms the data so that each data point lies on the surface of a unit hypersphere (a sphere with a radius of 1) centered at the origin. This technique is particularly useful when you want to emphasize the direction or pattern in the data rather than its magnitude.\n",
    "\n",
    "Normalization is achieved by dividing each data point by the Euclidean norm (L2 norm) of the feature vector. Mathematically, for a feature vector \"x,\" \n",
    "\n",
    "Here's a brief example to illustrate the Unit Vector (Normalization) technique:\n",
    "\n",
    "Suppose you have a dataset of two-dimensional points:\n",
    " [(3, 4), (1, 2), (-2, -2)]\n",
    "\n",
    "To normalize these points using the Unit Vector technique, you would:\n",
    "\n",
    "1. Calculate the L2 norm (Euclidean norm) for each point, which is the square root of the sum of the squares of its components. For example:\n",
    "   - For (3, 4):sqrt{3^2 + 4^2} = 5 \n",
    "   - For (1, 2):sqrt{1^2 + 2^2} = sqrt{5}\n",
    "   - For (-2, -2):sqrt{(-2)^2 + (-2)^2} =sqrt{2}\n",
    "\n",
    "2. Normalize each point by dividing it by its L2 norm:\n",
    "\n",
    "After normalization, the points now lie on the surface of a unit circle (for two-dimensional data) or a unit hypersphere (for higher-dimensional data), and their magnitudes are all equal to 1. This technique is useful when you want to compare the directions or patterns of different data points while eliminating the influence of their magnitudes. It's particularly common in machine learning algorithms that rely on distance measures or when dealing with features with varying scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3\n",
    "\n",
    "PCA, which stands for Principal Component Analysis, is a dimensionality reduction technique used in statistics and machine learning to reduce the number of features (dimensions) in a dataset while preserving as much of the original information as possible. It does this by transforming the original features into a new set of orthogonal (uncorrelated) features called principal components. These principal components are ordered in such a way that the first few components capture the most variance in the data, making them suitable for reducing the dimensionality of the dataset.\n",
    "\n",
    "Here's how PCA works:\n",
    "\n",
    "1. **Standardization**: First, standardize the dataset by subtracting the mean of each feature from the data points and scaling them by their standard deviation. This step ensures that all features have the same scale, which is crucial for PCA.\n",
    "\n",
    "2. **Covariance Matrix**: Calculate the covariance matrix of the standardized data. The covariance matrix describes the relationships between different features in the dataset.\n",
    "\n",
    "3. **Eigenvalue Decomposition**: Compute the eigenvalues and eigenvectors of the covariance matrix. The eigenvectors represent the principal components, and the eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "4. **Selecting Principal Components**: Sort the eigenvalues in descending order. The eigenvector corresponding to the largest eigenvalue is the first principal component, the one corresponding to the second-largest eigenvalue is the second principal component, and so on. You can choose to keep a certain number of top principal components based on the explained variance or the desired dimensionality reduction.\n",
    "\n",
    "5. **Projection**: Project the original data onto the selected principal components to obtain a lower-dimensional representation of the data.\n",
    "\n",
    "Here's a simple example to illustrate PCA:\n",
    "\n",
    "Suppose you have a dataset with two features, \"Height\" (in inches) and \"Weight\" (in pounds), for a group of individuals:\n",
    "\n",
    "```\n",
    "Height (inches) | Weight (pounds)\n",
    "--------------------------------\n",
    "    60          |      120\n",
    "    65          |      150\n",
    "    70          |      180\n",
    "    75          |      210\n",
    "```\n",
    "\n",
    "1. Standardize the data by subtracting the mean and dividing by the standard deviation for each feature.\n",
    "\n",
    "2. Calculate the covariance matrix of the standardized data:\n",
    "\n",
    "```\n",
    "Covariance Matrix:\n",
    "[[1.0  0.98]\n",
    " [0.98 1.0]]\n",
    "```\n",
    "\n",
    "3. Compute the eigenvalues and eigenvectors of the covariance matrix. Let's say we find the following eigenvalues and eigenvectors:\n",
    "\n",
    "```\n",
    "Eigenvalues: [1.98, 0.02]\n",
    "Eigenvectors:\n",
    "[0.707, -0.707]\n",
    "[0.707,  0.707]\n",
    "```\n",
    "\n",
    "4. Since the first eigenvalue (1.98) is much larger than the second (0.02), we choose the first eigenvector as the first principal component.\n",
    "\n",
    "5. Project the original data onto the first principal component to reduce it to one dimension:\n",
    "\n",
    "```\n",
    "Reduced Data:\n",
    "[ 1.41]\n",
    "[ 0.47]\n",
    "[-0.47]\n",
    "[-1.41]\n",
    "```\n",
    "\n",
    "Now, you have reduced the dimensionality of the dataset from two features (Height and Weight) to one principal component, capturing the most significant variation in the data. This reduced representation can be useful for visualization, analysis, or feeding into machine learning algorithms with fewer features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4\n",
    "\n",
    "    PCA (Principal Component Analysis) is closely related to feature extraction and can be used as a feature extraction technique. The main goal of feature extraction is to transform high-dimensional data into a lower-dimensional representation while preserving relevant information. PCA achieves this by identifying the most informative directions (principal components) in the data and projecting the original features onto these components, effectively reducing dimensionality.\n",
    "\n",
    "Here's how PCA is used for feature extraction:\n",
    "\n",
    "1. **Data Preprocessing**: Start with a dataset that has multiple features (dimensions). It's important to standardize the data to ensure that all features have the same scale, as PCA is sensitive to the scale of the features.\n",
    "\n",
    "2. **PCA Computation**: Apply PCA to the standardized data, which involves calculating the covariance matrix, finding its eigenvectors (principal components), and selecting a subset of these components based on the desired dimensionality reduction or explained variance.\n",
    "\n",
    "3. **Feature Projection**: Project the original data onto the selected principal components. This projection creates a new set of features, which are linear combinations of the original features. These new features are typically orthogonal (uncorrelated), and they capture the most significant information in the data.\n",
    "\n",
    "Here's an example to illustrate PCA as a feature extraction technique:\n",
    "\n",
    "Suppose you have a dataset with five features related to a person's education, income, work experience, age, and savings. You want to reduce the dimensionality of this dataset while preserving as much relevant information as possible. Here are the first few rows of the dataset:\n",
    "\n",
    "```\n",
    "Education (years) | Income ($) | Experience (years) | Age (years) | Savings ($)\n",
    "-------------------------------------------------------------------------------\n",
    "      16          |   60000    |         10          |     35      |   20000\n",
    "      14          |   40000    |          8          |     28      |   15000\n",
    "      18          |   75000    |         12          |     42      |   25000\n",
    "      12          |   30000    |          5          |     22      |   10000\n",
    "```\n",
    "\n",
    "1. Standardize the data by subtracting the mean and dividing by the standard deviation for each feature.\n",
    "\n",
    "2. Apply PCA to the standardized data. Let's say you choose to retain two principal components.\n",
    "\n",
    "3. PCA identifies the first two principal components, which are linear combinations of the original features. Let's denote these as PC1 and PC2.\n",
    "\n",
    "4. Project the original data onto PC1 and PC2 to obtain the reduced feature representation:\n",
    "\n",
    "```\n",
    "Reduced Features:\n",
    "    PC1         |     PC2\n",
    "------------------------------\n",
    "   -1.86       |    0.19\n",
    "   -0.48       |   -0.15\n",
    "    2.03       |   -0.08\n",
    "   -2.62       |    0.05\n",
    "```\n",
    "\n",
    "In this reduced feature representation, you've transformed the original five features into just two features (PC1 and PC2). These two features capture the most significant information in the data while reducing the dimensionality. You can now use PC1 and PC2 for analysis, visualization, or as inputs to machine learning algorithms, effectively achieving feature extraction through PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5\n",
    "\n",
    "To preprocess the dataset for building a recommendation system for a food delivery service using Min-Max scaling, you would follow these steps:\n",
    "\n",
    "1. **Understand the Data**: First, you should thoroughly understand the dataset and its features, including \"price,\" \"rating,\" and \"delivery time.\" Ensure you know the range and distribution of each feature to determine if scaling is necessary.\n",
    "\n",
    "\n",
    "2. **Apply Min-Max Scaling**: Once you've standardized (if needed), you can apply Min-Max scaling to rescale the features to a common range, typically between 0 and 1. Here's how to do it for each feature:\n",
    "\n",
    "\n",
    "3. **Use Scaled Features**: After applying Min-Max scaling to all relevant features, you'll have transformed the data so that all features have values between 0 and 1. These scaled features can now be used as inputs for your recommendation system.\n",
    "\n",
    "4. **Recommendation System Development**: With the preprocessed data, you can proceed to develop your recommendation system using techniques like collaborative filtering, content-based filtering, or hybrid methods. The scaled features will be used to calculate similarity scores or make predictions for recommendations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6\n",
    "\n",
    "Using PCA (Principal Component Analysis) to reduce the dimensionality of a dataset for predicting stock prices can be a valuable approach to manage high-dimensional data while retaining its essential information. Here's how you can apply PCA in the context of building a stock price prediction model:\n",
    "\n",
    "1. **Data Preparation**:\n",
    "   - Gather and preprocess your dataset, which includes features like company financial data and market trends. This may involve data cleaning, handling missing values, and ensuring that all features are on a similar scale.\n",
    "\n",
    "2. **Standardization**:\n",
    "   - Standardize the dataset by subtracting the mean and dividing by the standard deviation for each feature. Standardization is essential for PCA because it makes sure that all features have the same scale.\n",
    "\n",
    "3. **PCA Computation**:\n",
    "   - Calculate the covariance matrix of the standardized dataset. The covariance matrix describes the relationships between different features.\n",
    "\n",
    "4. **Eigenvalue Decomposition**:\n",
    "   - Compute the eigenvalues and eigenvectors of the covariance matrix. The eigenvectors represent the principal components, and the eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "5. **Selecting Principal Components**:\n",
    "   - Sort the eigenvalues in descending order. The eigenvector corresponding to the largest eigenvalue is the first principal component, the one corresponding to the second-largest eigenvalue is the second principal component, and so on.\n",
    "   - Decide on the number of principal components to retain based on your project's requirements, such as the desired level of dimensionality reduction or explained variance. You may plot the cumulative explained variance to help make this decision.\n",
    "\n",
    "6. **Projection**:\n",
    "   - Project the original dataset onto the selected principal components. This transformation creates a new dataset with reduced dimensions.\n",
    "\n",
    "7. **Model Building**:\n",
    "   - Train your stock price prediction model using the reduced-dimension dataset created through PCA. You can use various machine learning techniques, such as regression, time series analysis, or deep learning, depending on the nature of your problem.\n",
    "\n",
    "8. **Evaluation**:\n",
    "   - Evaluate the performance of your stock price prediction model using appropriate metrics, such as mean squared error (MSE) or root mean squared error (RMSE), and assess its ability to make accurate predictions.\n",
    "\n",
    "Benefits of using PCA for dimensionality reduction in the context of stock price prediction:\n",
    "\n",
    "- **Noise Reduction**: PCA can help remove noise and irrelevant features from the dataset, allowing the model to focus on the most informative aspects of the data.\n",
    "\n",
    "- **Computational Efficiency**: With a reduced number of features, training and evaluating models can be computationally more efficient, especially when dealing with a large number of initial features.\n",
    "\n",
    "- **Interpretability**: A smaller set of principal components can be easier to interpret than a large number of original features, which can be important for understanding the driving factors behind stock price movements.\n",
    "\n",
    "- **Mitigating Multicollinearity**: PCA can address multicollinearity (high correlation between features) by creating orthogonal principal components, which can lead to more stable and interpretable models.\n",
    "\n",
    "Keep in mind that while PCA can be a powerful technique for dimensionality reduction, it's essential to strike a balance between reducing dimensionality and retaining enough information to make accurate predictions. Experiment with different numbers of principal components and evaluate the impact on model performance to determine the optimal dimensionality reduction strategy for your specific stock price prediction project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[1, 5, 10, 15, 20]\n",
    "min=1\n",
    "max=20\n",
    "normalized=[ (i-min)/(max-min) for i in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.21052631578947367, 0.47368421052631576, 0.7368421052631579, 1.0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7\n",
    "Determining how many principal components to retain when performing feature extraction using PCA is a critical decision that depends on the specific goals of your analysis and the amount of variance you want to preserve. Here's how you can make an informed decision on the number of principal components to retain for the given dataset containing features: [height, weight, age, gender, blood pressure]:\n",
    "\n",
    "1. **Standardization**: Start by standardizing your dataset to ensure that all features have the same scale. This step is crucial because PCA is sensitive to feature scales.\n",
    "\n",
    "2. **PCA Calculation**: Calculate the covariance matrix of the standardized data and compute the eigenvalues and eigenvectors.\n",
    "\n",
    "3. **Eigenvalue Analysis**: Examine the eigenvalues obtained from the PCA. Eigenvalues represent the amount of variance explained by each principal component. Typically, you'll find that the eigenvalues are sorted in descending order.\n",
    "\n",
    "4. **Explained Variance**: Plot the cumulative explained variance ratio as a function of the number of principal components. This plot will show how much variance is preserved as you increase the number of components. The explained variance ratio for each principal component is given by its eigenvalue divided by the sum of all eigenvalues.\n",
    "\n",
    "5. **Threshold**: Choose a threshold for the amount of variance you want to retain. This threshold could be a specific percentage of the total variance you aim to preserve, such as 95% or 99%. Alternatively, you can select a fixed number of principal components that capture a significant amount of variance.\n",
    "\n",
    "6. **Decision**: Based on the cumulative explained variance plot and your chosen threshold, decide how many principal components to retain. You can choose the number that captures enough variance to meet your analysis goals.\n",
    "\n",
    "The decision of how many principal components to retain depends on your specific objectives and trade-offs:\n",
    "\n",
    "- If you want to reduce dimensionality significantly while preserving most of the information, you might choose a high threshold like 95% or 99% of the variance.\n",
    "  \n",
    "- If you are interested in reducing dimensionality but are willing to retain a bit more information, you might choose a threshold like 90% or 80%.\n",
    "\n",
    "- If you are concerned about interpretability and only want to simplify the dataset while retaining as much information as possible, you might choose a threshold close to 100%.\n",
    "\n",
    "- You can also use a scree plot (plot of eigenvalues) to visually inspect where there is an \"elbow\" in the plot, which can provide a clue on how many components to retain.\n",
    "\n",
    "Ultimately, the choice of how many principal components to retain should align with your analysis goals, the trade-offs you're willing to make in terms of dimensionality reduction, and the amount of variance you consider essential to preserve for your particular application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
