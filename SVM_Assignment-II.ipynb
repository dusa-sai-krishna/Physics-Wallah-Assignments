{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1\n",
    "\n",
    "In machine learning, kernel functions are a mathematical technique that allows us to implicitly transform data into a higher-dimensional space, where it may be easier to find linear decision boundaries. Polynomial functions are one type of kernel function commonly used in this context.\n",
    "\n",
    "The relationship between polynomial functions and kernel functions is that polynomial kernels, specifically the polynomial kernel function, are a type of kernel function. The polynomial kernel calculates the similarity between data points in the original feature space by applying a polynomial function to their dot product. It's used to map data into a higher-dimensional space using polynomial transformations. Other kernel functions, like the Gaussian (RBF) kernel, are also used to map data into higher-dimensional spaces, but polynomial kernels specifically employ polynomial functions for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load your dataset\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "# Create an SVM classifier with a polynomial kernel\n",
    "svm_classifier = SVC(kernel='poly', degree=3)  # You can adjust the 'degree' parameter for the polynomial order\n",
    "\n",
    "# Fit the SVM classifier to your data\n",
    "svm_classifier.fit(X, y)\n",
    "\n",
    "# Make predictions using the trained SVM\n",
    "predictions = svm_classifier.predict(X)\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3\n",
    "\n",
    "In Support Vector Regression (SVR), the value of epsilon (ε) determines the width of the tube within which errors are ignored. Increasing the value of epsilon typically leads to an increase in the number of support vectors. Here's how it works:\n",
    "\n",
    "1. **Smaller Epsilon (Tighter Tube):** When you set a smaller epsilon value, you are specifying a narrower tube around the regression line. This means that the SVR model will be less tolerant of errors or deviations from the regression line. As a result, it is more likely to include data points as support vectors to ensure that errors stay within the smaller tube. This leads to a larger number of support vectors, and the model may fit the training data more closely.\n",
    "\n",
    "2. **Larger Epsilon (Wider Tube):** Conversely, when you set a larger epsilon value, you are allowing a wider tube around the regression line, meaning that the model can tolerate larger errors. In this case, fewer data points are included as support vectors because the model has more flexibility to accommodate larger deviations from the regression line. This results in a smaller number of support vectors.\n",
    "\n",
    "The choice of epsilon in SVR is a trade-off between model complexity and fitting accuracy. A smaller epsilon may lead to a model that fits the training data more closely but might be sensitive to noise or overfit. A larger epsilon allows for a more robust model that is less sensitive to individual data points but may have a looser fit to the training data. The optimal epsilon value depends on the specific characteristics of your data and the trade-offs you are willing to make between model complexity and fitting accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4\n",
    "\n",
    "1. **Kernel Function:** The choice of kernel function affects the mapping of the data into a higher-dimensional space. Different kernel functions (e.g., linear, polynomial, radial basis function) can capture different types of relationships in the data. For example, a polynomial kernel may work well for data with polynomial patterns, while an RBF kernel is more flexible and can capture complex non-linear relationships.\n",
    "\n",
    "2. **C Parameter (Regularization):** The C parameter controls the trade-off between maximizing the margin and minimizing the classification error. A small C makes the decision boundary smoother, allowing some misclassifications, while a large C results in a narrower margin but fewer misclassifications. You might increase C when you want to reduce misclassifications (potentially overfit), or decrease it to encourage a wider margin (potentially underfit).\n",
    "\n",
    "3. **Epsilon Parameter (for Epsilon-Support Vector Regression, ε-SVR):** The epsilon parameter (ε) defines a tube around the regression line within which errors are ignored. It controls the sensitivity to errors. Larger ε allows more errors within the tube, while smaller ε enforces tighter tolerance to errors. You might increase ε when you have noisy data or decrease it for a more precise fit.\n",
    "\n",
    "4. **Gamma Parameter (for RBF Kernel):** In the context of the Radial Basis Function (RBF) kernel, the gamma parameter defines the shape of the decision boundary. A small gamma results in a more flexible boundary, capturing fine details in the data, which can lead to overfitting. A large gamma results in a smoother boundary, potentially underfitting the data. You might increase gamma when the data is complex, and you want a more intricate decision boundary, or decrease it for a smoother, less complex boundary.\n",
    "\n",
    "The choice of these parameters should be made through experimentation and model validation, as the optimal values depend on the specific dataset and problem you're working with. Adjusting these parameters allows you to fine-tune your SVR model to achieve the best balance between model complexity, accuracy, and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5\n",
    "\n",
    "\n",
    "IRIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset\n",
    "from sklearn.datasets import load_iris\n",
    "data=load_iris()\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n"
     ]
    }
   ],
   "source": [
    "X=data.data\n",
    "y=data.target\n",
    "print(X.shape,y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "X_scaled=scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_scaled,y,test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier=SVC()\n",
    "classifier.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  0  0]\n",
      " [ 0 12  0]\n",
      " [ 0  2 15]]\n",
      "0.9555555555555556\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       0.86      1.00      0.92        12\n",
      "           2       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.96        45\n",
      "   macro avg       0.95      0.96      0.95        45\n",
      "weighted avg       0.96      0.96      0.96        45\n",
      "\n",
      "0.953525641025641\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report,f1_score\n",
    "y_pred=classifier.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(f1_score(y_test,y_pred,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "parameters={ 'C':np.arange(1,10,0.5),\n",
    "            'kernel':['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            'degree':np.arange(1,10,0.5),\n",
    "            'gamma':['scale','auto']\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "classifier=SVC()\n",
    "grid=GridSearchCV(classifier,param_grid=parameters,scoring='accuracy',cv=5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={'C': array([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. , 5.5, 6. , 6.5, 7. ,\n",
       "       7.5, 8. , 8.5, 9. , 9.5]),\n",
       "                         'degree': array([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. , 5.5, 6. , 6.5, 7. ,\n",
       "       7.5, 8. , 8.5, 9. , 9.5]),\n",
       "                         'gamma': ['scale', 'auto'],\n",
       "                         'kernel': ['linear', 'poly', 'rbf', 'sigmoid']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 5.0, 'degree': 1.0, 'gamma': 'auto', 'kernel': 'poly'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9714285714285715"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9777777777777777"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's save the model on best params\n",
    "classifier=SVC(C=5,degree=1,gamma='auto',kernel='poly')\n",
    "classifier.fit(X_train,y_train)\n",
    "accuracy_score(y_test,classifier.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pik\n",
    "pik.dump(classifier,open('SVC.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
